stage -1: Data Download
stage 0: Data preparation
utils/validate_data_dir.sh: Successfully validated data-directory data/dev_clean
local/data_prep.sh: successfully prepared data in data/dev_clean
utils/validate_data_dir.sh: Successfully validated data-directory data/test_clean
local/data_prep.sh: successfully prepared data in data/test_clean
utils/validate_data_dir.sh: Successfully validated data-directory data/dev_other
local/data_prep.sh: successfully prepared data in data/dev_other
utils/validate_data_dir.sh: Successfully validated data-directory data/test_other
local/data_prep.sh: successfully prepared data in data/test_other
utils/validate_data_dir.sh: Successfully validated data-directory data/train_clean_100
local/data_prep.sh: successfully prepared data in data/train_clean_100
utils/validate_data_dir.sh: Successfully validated data-directory data/train_clean_360
local/data_prep.sh: successfully prepared data in data/train_clean_360
utils/validate_data_dir.sh: Successfully validated data-directory data/train_other_500
local/data_prep.sh: successfully prepared data in data/train_other_500
stage 1: Feature Generation
steps/make_fbank_pitch.sh --cmd run.pl --nj 188 --write_utt2num_frames true data/dev_clean exp/make_fbank/dev_clean fbank
steps/make_fbank_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
steps/make_fbank_pitch.sh: Succeeded creating filterbank and pitch features for dev_clean
fix_data_dir.sh: kept all 2703 utterances.
fix_data_dir.sh: old files are kept in data/dev_clean/.backup
steps/make_fbank_pitch.sh --cmd run.pl --nj 188 --write_utt2num_frames true data/test_clean exp/make_fbank/test_clean fbank
steps/make_fbank_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
steps/make_fbank_pitch.sh: Succeeded creating filterbank and pitch features for test_clean
fix_data_dir.sh: kept all 2620 utterances.
fix_data_dir.sh: old files are kept in data/test_clean/.backup
steps/make_fbank_pitch.sh --cmd run.pl --nj 188 --write_utt2num_frames true data/dev_other exp/make_fbank/dev_other fbank
steps/make_fbank_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
steps/make_fbank_pitch.sh: Succeeded creating filterbank and pitch features for dev_other
fix_data_dir.sh: kept all 2864 utterances.
fix_data_dir.sh: old files are kept in data/dev_other/.backup
steps/make_fbank_pitch.sh --cmd run.pl --nj 188 --write_utt2num_frames true data/test_other exp/make_fbank/test_other fbank
steps/make_fbank_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
steps/make_fbank_pitch.sh: Succeeded creating filterbank and pitch features for test_other
fix_data_dir.sh: kept all 2939 utterances.
fix_data_dir.sh: old files are kept in data/test_other/.backup
steps/make_fbank_pitch.sh --cmd run.pl --nj 188 --write_utt2num_frames true data/train_clean_100 exp/make_fbank/train_clean_100 fbank
steps/make_fbank_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
steps/make_fbank_pitch.sh: Succeeded creating filterbank and pitch features for train_clean_100
fix_data_dir.sh: kept all 28539 utterances.
fix_data_dir.sh: old files are kept in data/train_clean_100/.backup
steps/make_fbank_pitch.sh --cmd run.pl --nj 188 --write_utt2num_frames true data/train_clean_360 exp/make_fbank/train_clean_360 fbank
steps/make_fbank_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
steps/make_fbank_pitch.sh: Succeeded creating filterbank and pitch features for train_clean_360
fix_data_dir.sh: kept all 104014 utterances.
fix_data_dir.sh: old files are kept in data/train_clean_360/.backup
steps/make_fbank_pitch.sh --cmd run.pl --nj 188 --write_utt2num_frames true data/train_other_500 exp/make_fbank/train_other_500 fbank
steps/make_fbank_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
steps/make_fbank_pitch.sh: Succeeded creating filterbank and pitch features for train_other_500
fix_data_dir.sh: kept all 148688 utterances.
fix_data_dir.sh: old files are kept in data/train_other_500/.backup
utils/combine_data.sh --extra_files utt2num_frames data/train_960_org data/train_clean_100 data/train_clean_360 data/train_other_500
utils/combine_data.sh [info]: not combining utt2uniq as it does not exist
utils/combine_data.sh [info]: not combining segments as it does not exist
utils/combine_data.sh: combined utt2spk
utils/combine_data.sh [info]: not combining utt2lang as it does not exist
utils/combine_data.sh: combined utt2dur
utils/combine_data.sh: combined utt2num_frames
utils/combine_data.sh [info]: not combining reco2dur as it does not exist
utils/combine_data.sh: combined feats.scp
utils/combine_data.sh: combined text
utils/combine_data.sh [info]: not combining cmvn.scp as it does not exist
utils/combine_data.sh [info]: not combining vad.scp as it does not exist
utils/combine_data.sh [info]: not combining reco2file_and_channel as it does not exist
utils/combine_data.sh: combined wav.scp
utils/combine_data.sh: combined spk2gender
utils/combine_data.sh: combined utt2num_frames
fix_data_dir.sh: kept all 281241 utterances.
fix_data_dir.sh: old files are kept in data/train_960_org/.backup
utils/combine_data.sh --extra_files utt2num_frames data/dev_org data/dev_clean data/dev_other
utils/combine_data.sh [info]: not combining utt2uniq as it does not exist
utils/combine_data.sh [info]: not combining segments as it does not exist
utils/combine_data.sh: combined utt2spk
utils/combine_data.sh [info]: not combining utt2lang as it does not exist
utils/combine_data.sh: combined utt2dur
utils/combine_data.sh: combined utt2num_frames
utils/combine_data.sh [info]: not combining reco2dur as it does not exist
utils/combine_data.sh: combined feats.scp
utils/combine_data.sh: combined text
utils/combine_data.sh [info]: not combining cmvn.scp as it does not exist
utils/combine_data.sh [info]: not combining vad.scp as it does not exist
utils/combine_data.sh [info]: not combining reco2file_and_channel as it does not exist
utils/combine_data.sh: combined wav.scp
utils/combine_data.sh: combined spk2gender
utils/combine_data.sh: combined utt2num_frames
fix_data_dir.sh: kept all 5567 utterances.
fix_data_dir.sh: old files are kept in data/dev_org/.backup
utils/data/get_reco2dur.sh: data/train_960_org/wav.scp indexed by utt-id; copying utt2dur to reco2dur
utils/perturb_data_dir_speed.sh: generated speed-perturbed version of data in data/train_960_org, in data/temp1
utils/validate_data_dir.sh: Successfully validated data-directory data/temp1
utils/perturb_data_dir_speed.sh: generated speed-perturbed version of data in data/train_960_org, in data/temp2
utils/validate_data_dir.sh: Successfully validated data-directory data/temp2
utils/perturb_data_dir_speed.sh: generated speed-perturbed version of data in data/train_960_org, in data/temp3
utils/validate_data_dir.sh: Successfully validated data-directory data/temp3
utils/combine_data.sh --extra-files utt2uniq data/train_sp_org data/temp1 data/temp2 data/temp3
utils/combine_data.sh: combined utt2uniq
utils/combine_data.sh [info]: not combining segments as it does not exist
utils/combine_data.sh: combined utt2spk
utils/combine_data.sh [info]: not combining utt2lang as it does not exist
utils/combine_data.sh: combined utt2dur
utils/combine_data.sh [info]: not combining utt2num_frames as it does not exist
utils/combine_data.sh: combined reco2dur
utils/combine_data.sh [info]: not combining feats.scp as it does not exist
utils/combine_data.sh: combined text
utils/combine_data.sh [info]: not combining cmvn.scp as it does not exist
utils/combine_data.sh [info]: not combining vad.scp as it does not exist
utils/combine_data.sh [info]: not combining reco2file_and_channel as it does not exist
utils/combine_data.sh: combined wav.scp
utils/combine_data.sh: combined spk2gender
fix_data_dir.sh: kept all 843723 utterances.
fix_data_dir.sh: old files are kept in data/train_sp_org/.backup
extract utterances having less than 3000 or more than 10 frames
utils/data/get_utt2num_frames.sh: data/train_960_org/utt2num_frames already present!
extract utterances having less than 400 or more than 0 characters
Reduced #utt from 281241 to 281231
fix_data_dir.sh: kept all 281231 utterances.
fix_data_dir.sh: old files are kept in data/train_960/.backup
change from 281241 to 281231
extract utterances having less than 3000 or more than 10 frames
utils/data/get_utt2dur.sh: data/train_sp_org/utt2dur already exists with the expected length.  We won't recompute it.
extract utterances having less than 400 or more than 0 characters
Reduced #utt from 843723 to 843692
fix_data_dir.sh: kept all 843692 utterances.
fix_data_dir.sh: old files are kept in data/train_sp/.backup
wc: data/train_sp_org/feats.scp: No such file or directory
wc: data/train_sp/feats.scp: No such file or directory
change from  to 
extract utterances having less than 3000 or more than 10 frames
utils/data/get_utt2num_frames.sh: data/dev_org/utt2num_frames already present!
extract utterances having less than 400 or more than 0 characters
Reduced #utt from 5567 to 5542
fix_data_dir.sh: kept all 5542 utterances.
fix_data_dir.sh: old files are kept in data/dev/.backup
change from 5567 to 5542
steps/make_fbank_pitch.sh --cmd run.pl --nj 188 --write_utt2num_frames true data/train_sp exp/make_fbank/train_sp fbank
steps/make_fbank_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
steps/make_fbank_pitch.sh: Succeeded creating filterbank and pitch features for train_sp
fix_data_dir.sh: kept all 843692 utterances.
fix_data_dir.sh: old files are kept in data/train_sp/.backup
compute-cmvn-stats scp:data/train_sp/feats.scp data/train_sp/cmvn.ark 
LOG (compute-cmvn-stats[5.5]:main():compute-cmvn-stats.cc:168) Wrote global CMVN stats to data/train_sp/cmvn.ark
LOG (compute-cmvn-stats[5.5]:main():compute-cmvn-stats.cc:171) Done accumulating CMVN stats for 843692 utterances; 0 had errors.
/apdcephfs/share_1149801/speech_user/tomasyu/jinchuan/E2E-ASR-Framework/egs/librispeech/espnet_utils/dump.sh --cmd run.pl --nj 188 --do_delta false data/train_sp/feats.scp data/train_sp/cmvn.ark exp/dump_feats/train dump/train_sp/deltafalse
/apdcephfs/share_1149801/speech_user/tomasyu/jinchuan/E2E-ASR-Framework/egs/librispeech/espnet_utils/dump.sh --cmd run.pl --nj 188 --do_delta false data/dev/feats.scp data/train_sp/cmvn.ark exp/dump_feats/dev dump/dev/deltafalse
/apdcephfs/share_1149801/speech_user/tomasyu/jinchuan/E2E-ASR-Framework/egs/librispeech/espnet_utils/dump.sh --cmd run.pl --nj 188 --do_delta false data/test_clean/feats.scp data/train_sp/cmvn.ark exp/dump_feats/recog/test_clean dump/test_clean/deltafalse
/apdcephfs/share_1149801/speech_user/tomasyu/jinchuan/E2E-ASR-Framework/egs/librispeech/espnet_utils/dump.sh --cmd run.pl --nj 188 --do_delta false data/test_other/feats.scp data/train_sp/cmvn.ark exp/dump_feats/recog/test_other dump/test_other/deltafalse
/apdcephfs/share_1149801/speech_user/tomasyu/jinchuan/E2E-ASR-Framework/egs/librispeech/espnet_utils/dump.sh --cmd run.pl --nj 188 --do_delta false data/dev_clean/feats.scp data/train_sp/cmvn.ark exp/dump_feats/recog/dev_clean dump/dev_clean/deltafalse
/apdcephfs/share_1149801/speech_user/tomasyu/jinchuan/E2E-ASR-Framework/egs/librispeech/espnet_utils/dump.sh --cmd run.pl --nj 188 --do_delta false data/dev_other/feats.scp data/train_sp/cmvn.ark exp/dump_feats/recog/dev_other dump/dev_other/deltafalse
dictionary: data/lang_char/train_960_unigram5000_units.txt
stage 2: Dictionary and Json Data Preparation
sentencepiece_trainer.cc(116) LOG(INFO) Running command: --input=data/lang_char/input.txt --vocab_size=5000 --model_type=unigram --model_prefix=data/lang_char/train_960_unigram5000 --input_sentence_size=100000000
sentencepiece_trainer.cc(49) LOG(INFO) Starts training with : 
TrainerSpec {
  input: data/lang_char/input.txt
  input_format: 
  model_prefix: data/lang_char/train_960_unigram5000
  model_type: UNIGRAM
  vocab_size: 5000
  self_test_sample_size: 0
  character_coverage: 0.9995
  input_sentence_size: 100000000
  shuffle_input_sentence: 1
  seed_sentencepiece_size: 1000000
  shrinking_factor: 0.75
  max_sentence_length: 4192
  num_threads: 16
  num_sub_iterations: 2
  max_sentencepiece_length: 16
  split_by_unicode_script: 1
  split_by_number: 1
  split_by_whitespace: 1
  treat_whitespace_as_suffix: 0
  hard_vocab_limit: 1
  use_all_vocab: 0
  unk_id: 0
  bos_id: 1
  eos_id: 2
  pad_id: -1
  unk_piece: <unk>
  bos_piece: <s>
  eos_piece: </s>
  pad_piece: <pad>
  unk_surface:  ⁇ 
}
NormalizerSpec {
  name: nmt_nfkc
  add_dummy_prefix: 1
  remove_extra_whitespaces: 1
  escape_whitespaces: 1
  normalization_rule_tsv: 
}

trainer_interface.cc(267) LOG(INFO) Loading corpus: data/lang_char/input.txt
trainer_interface.cc(315) LOG(INFO) Loaded all 281231 sentences
trainer_interface.cc(330) LOG(INFO) Adding meta_piece: <unk>
trainer_interface.cc(330) LOG(INFO) Adding meta_piece: <s>
trainer_interface.cc(330) LOG(INFO) Adding meta_piece: </s>
trainer_interface.cc(335) LOG(INFO) Normalizing sentences...
trainer_interface.cc(384) LOG(INFO) all chars count=50059099
trainer_interface.cc(392) LOG(INFO) Done: 99.9546% characters are covered.
trainer_interface.cc(402) LOG(INFO) Alphabet size=27
trainer_interface.cc(403) LOG(INFO) Final character coverage=0.999546
trainer_interface.cc(435) LOG(INFO) Done! preprocessed 281231 sentences.
unigram_model_trainer.cc(129) LOG(INFO) Making suffix array...
unigram_model_trainer.cc(133) LOG(INFO) Extracting frequent sub strings...
unigram_model_trainer.cc(184) LOG(INFO) Initialized 213257 seed sentencepieces
trainer_interface.cc(441) LOG(INFO) Tokenizing input sentences with whitespace: 281231
trainer_interface.cc(451) LOG(INFO) Done! 89111
unigram_model_trainer.cc(470) LOG(INFO) Using 89111 sentences for EM training
unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=77227 obj=9.24967 num_tokens=159660 num_tokens/piece=2.06741
unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=55588 obj=7.35924 num_tokens=159443 num_tokens/piece=2.8683
unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=41691 obj=7.2969 num_tokens=170470 num_tokens/piece=4.08889
unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=41680 obj=7.28984 num_tokens=170429 num_tokens/piece=4.08899
unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=31260 obj=7.32334 num_tokens=191459 num_tokens/piece=6.12473
unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=31260 obj=7.31542 num_tokens=191414 num_tokens/piece=6.12329
unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=23445 obj=7.38282 num_tokens=212963 num_tokens/piece=9.08351
unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=23445 obj=7.36897 num_tokens=212941 num_tokens/piece=9.08258
unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=17583 obj=7.47076 num_tokens=232289 num_tokens/piece=13.211
unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=17583 obj=7.4506 num_tokens=232259 num_tokens/piece=13.2093
unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=13187 obj=7.58795 num_tokens=249990 num_tokens/piece=18.9573
unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=13187 obj=7.56229 num_tokens=249999 num_tokens/piece=18.958
unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=9890 obj=7.73384 num_tokens=266920 num_tokens/piece=26.9889
unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=9890 obj=7.70253 num_tokens=266933 num_tokens/piece=26.9902
unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=7417 obj=7.9019 num_tokens=283044 num_tokens/piece=38.1615
unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=7417 obj=7.86548 num_tokens=283076 num_tokens/piece=38.1658
unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=5562 obj=8.09737 num_tokens=297673 num_tokens/piece=53.5191
unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=5562 obj=8.05578 num_tokens=297684 num_tokens/piece=53.521
unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=5500 obj=8.06434 num_tokens=298712 num_tokens/piece=54.3113
unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=5500 obj=8.06214 num_tokens=298729 num_tokens/piece=54.3144
trainer_interface.cc(507) LOG(INFO) Saving model: data/lang_char/train_960_unigram5000.model
trainer_interface.cc(531) LOG(INFO) Saving vocabs: data/lang_char/train_960_unigram5000.vocab
processed 10000 lines
processed 20000 lines
processed 30000 lines
processed 40000 lines
processed 50000 lines
processed 60000 lines
processed 70000 lines
processed 80000 lines
processed 90000 lines
processed 100000 lines
processed 110000 lines
processed 120000 lines
processed 130000 lines
processed 140000 lines
processed 150000 lines
processed 160000 lines
processed 170000 lines
processed 180000 lines
processed 190000 lines
processed 200000 lines
processed 210000 lines
processed 220000 lines
processed 230000 lines
processed 240000 lines
processed 250000 lines
processed 260000 lines
processed 270000 lines
processed 280000 lines
skipped 0 empty lines
filtered 0 lines
5000 data/lang_char/train_960_unigram5000_units.txt
/apdcephfs/share_1149801/speech_user/tomasyu/jinchuan/E2E-ASR-Framework/egs/librispeech/espnet_utils/data2json.sh --nj 188 --feat dump/train_sp/deltafalse/feats.scp --bpecode data/lang_char/train_960_unigram5000.model data/train_sp data/lang_char/train_960_unigram5000_units.txt
/apdcephfs/share_1149801/speech_user/tomasyu/jinchuan/E2E-ASR-Framework/egs/librispeech/espnet_utils/feat_to_shape.sh --cmd run.pl --nj 188 --filetype  --preprocess-conf  --verbose 0 dump/train_sp/deltafalse/feats.scp data/train_sp/tmp-2Ybpk/input_1/shape.scp
processed 10000 lines
processed 20000 lines
processed 30000 lines
processed 40000 lines
processed 50000 lines
processed 60000 lines
processed 70000 lines
processed 80000 lines
processed 90000 lines
processed 100000 lines
processed 110000 lines
processed 120000 lines
processed 130000 lines
processed 140000 lines
processed 150000 lines
processed 160000 lines
processed 170000 lines
processed 180000 lines
processed 190000 lines
processed 200000 lines
processed 210000 lines
processed 220000 lines
processed 230000 lines
processed 240000 lines
processed 250000 lines
processed 260000 lines
processed 270000 lines
processed 280000 lines
processed 290000 lines
processed 300000 lines
processed 310000 lines
processed 320000 lines
processed 330000 lines
processed 340000 lines
processed 350000 lines
processed 360000 lines
processed 370000 lines
processed 380000 lines
processed 390000 lines
processed 400000 lines
processed 410000 lines
processed 420000 lines
processed 430000 lines
processed 440000 lines
processed 450000 lines
processed 460000 lines
processed 470000 lines
processed 480000 lines
processed 490000 lines
processed 500000 lines
processed 510000 lines
processed 520000 lines
processed 530000 lines
processed 540000 lines
processed 550000 lines
processed 560000 lines
processed 570000 lines
processed 580000 lines
processed 590000 lines
processed 600000 lines
processed 610000 lines
processed 620000 lines
processed 630000 lines
processed 640000 lines
processed 650000 lines
processed 660000 lines
processed 670000 lines
processed 680000 lines
processed 690000 lines
processed 700000 lines
processed 710000 lines
processed 720000 lines
processed 730000 lines
processed 740000 lines
processed 750000 lines
processed 760000 lines
processed 770000 lines
processed 780000 lines
processed 790000 lines
processed 800000 lines
processed 810000 lines
processed 820000 lines
processed 830000 lines
processed 840000 lines
skipped 0 empty lines
filtered 0 lines
/apdcephfs/share_1149801/speech_user/tomasyu/jinchuan/E2E-ASR-Framework/egs/librispeech/espnet_utils/data2json.sh --nj 188 --feat dump/dev/deltafalse/feats.scp --bpecode data/lang_char/train_960_unigram5000.model data/dev data/lang_char/train_960_unigram5000_units.txt
/apdcephfs/share_1149801/speech_user/tomasyu/jinchuan/E2E-ASR-Framework/egs/librispeech/espnet_utils/feat_to_shape.sh --cmd run.pl --nj 188 --filetype  --preprocess-conf  --verbose 0 dump/dev/deltafalse/feats.scp data/dev/tmp-HX7x8/input_1/shape.scp
skipped 0 empty lines
filtered 0 lines
/apdcephfs/share_1149801/speech_user/tomasyu/jinchuan/E2E-ASR-Framework/egs/librispeech/espnet_utils/data2json.sh --nj 188 --feat dump/test_clean/deltafalse/feats.scp --bpecode data/lang_char/train_960_unigram5000.model data/test_clean data/lang_char/train_960_unigram5000_units.txt
/apdcephfs/share_1149801/speech_user/tomasyu/jinchuan/E2E-ASR-Framework/egs/librispeech/espnet_utils/feat_to_shape.sh --cmd run.pl --nj 188 --filetype  --preprocess-conf  --verbose 0 dump/test_clean/deltafalse/feats.scp data/test_clean/tmp-WBAqL/input_1/shape.scp
skipped 0 empty lines
filtered 0 lines
/apdcephfs/share_1149801/speech_user/tomasyu/jinchuan/E2E-ASR-Framework/egs/librispeech/espnet_utils/data2json.sh --nj 188 --feat dump/test_other/deltafalse/feats.scp --bpecode data/lang_char/train_960_unigram5000.model data/test_other data/lang_char/train_960_unigram5000_units.txt
/apdcephfs/share_1149801/speech_user/tomasyu/jinchuan/E2E-ASR-Framework/egs/librispeech/espnet_utils/feat_to_shape.sh --cmd run.pl --nj 188 --filetype  --preprocess-conf  --verbose 0 dump/test_other/deltafalse/feats.scp data/test_other/tmp-Z4Pia/input_1/shape.scp
skipped 0 empty lines
filtered 0 lines
/apdcephfs/share_1149801/speech_user/tomasyu/jinchuan/E2E-ASR-Framework/egs/librispeech/espnet_utils/data2json.sh --nj 188 --feat dump/dev_clean/deltafalse/feats.scp --bpecode data/lang_char/train_960_unigram5000.model data/dev_clean data/lang_char/train_960_unigram5000_units.txt
/apdcephfs/share_1149801/speech_user/tomasyu/jinchuan/E2E-ASR-Framework/egs/librispeech/espnet_utils/feat_to_shape.sh --cmd run.pl --nj 188 --filetype  --preprocess-conf  --verbose 0 dump/dev_clean/deltafalse/feats.scp data/dev_clean/tmp-m1KZd/input_1/shape.scp
skipped 0 empty lines
filtered 0 lines
/apdcephfs/share_1149801/speech_user/tomasyu/jinchuan/E2E-ASR-Framework/egs/librispeech/espnet_utils/data2json.sh --nj 188 --feat dump/dev_other/deltafalse/feats.scp --bpecode data/lang_char/train_960_unigram5000.model data/dev_other data/lang_char/train_960_unigram5000_units.txt
/apdcephfs/share_1149801/speech_user/tomasyu/jinchuan/E2E-ASR-Framework/egs/librispeech/espnet_utils/feat_to_shape.sh --cmd run.pl --nj 188 --filetype  --preprocess-conf  --verbose 0 dump/dev_other/deltafalse/feats.scp data/dev_other/tmp-JCjhX/input_1/shape.scp
/apdcephfs/share_1149801/speech_user/tomasyu/jinchuan/E2E-ASR-Framework/egs/librispeech/espnet_utils/feat_to_shape.sh: line 65: 14462 Terminated              ${cmd} JOB=1:${nj} ${logdir}/feat_to_shape.JOB.log feat-to-shape.py --verbose ${verbose} ${preprocess_opt} ${filetype_opt} scp:${logdir}/feats.JOB.scp ${logdir}/shape.JOB.scp
